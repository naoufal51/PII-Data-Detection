{"cells":[{"cell_type":"markdown","metadata":{},"source":["###  This code is a modification of mutiple other notebook from the competition\n","Will document further ....\n","\n","This is a notebook used to train an PII detection model based on a NER approach. We train our transformers model on both original essays and LLM generated ones to achieve higher F5 score.\n","The data is readily accessible in the kaggle competition page, if you want to train the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q  seqeval evaluate\n","!pip install -U -q datasets accelerate \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:31:37.034496Z","iopub.status.busy":"2024-01-28T09:31:37.033631Z","iopub.status.idle":"2024-01-28T09:31:43.108989Z","shell.execute_reply":"2024-01-28T09:31:43.107979Z","shell.execute_reply.started":"2024-01-28T09:31:37.034449Z"},"trusted":true},"outputs":[],"source":["\n","import json\n","import numpy as np\n","\n","data = json.load(open(\"./data/train.json\"))\n","\n","# downsampling of negative examples\n","p=[] # positive samples (contain relevant labels)\n","n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n","for d in data:\n","    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n","    else: n.append(d)\n","print(\"original datapoints: \", len(data))\n","\n","external = json.load(open(\"./data/pii_dataset_fixed.json\"))\n","print(\"external datapoints: \", len(external))\n","\n","moredata = json.load(open(\"./data/moredata_dataset_fixed.json\"))\n","print(\"moredata datapoints: \", len(moredata))\n","\n","train_data = moredata+external+p+n[:len(n)//3]\n","print(\"combined: \", len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:31:43.111805Z","iopub.status.busy":"2024-01-28T09:31:43.111466Z","iopub.status.idle":"2024-01-28T09:31:49.697756Z","shell.execute_reply":"2024-01-28T09:31:49.696673Z","shell.execute_reply.started":"2024-01-28T09:31:43.111778Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","import numpy as np\n","import random\n","from seqeval.metrics import recall_score, precision_score\n","from seqeval.metrics import classification_report\n","from seqeval.metrics import f1_score\n","\n","def tokenize(example, tokenizer, label2id, max_length):\n","\n","    # rebuild text from tokens\n","    text = []\n","    labels = []\n","\n","    for t, l, ws in zip(\n","        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n","    ):\n","        text.append(t)\n","        labels.extend([l] * len(t))\n","\n","        if ws:\n","            text.append(\" \")\n","            labels.append(\"O\")\n","\n","    # actual tokenization\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n","\n","    labels = np.array(labels)\n","\n","    text = \"\".join(text)\n","    token_labels = []\n","\n","    for start_idx, end_idx in tokenized.offset_mapping:\n","        # CLS token\n","        if start_idx == 0 and end_idx == 0:\n","            token_labels.append(label2id[\"O\"])\n","            continue\n","\n","        # case when token starts with whitespace\n","        if text[start_idx].isspace():\n","            start_idx += 1\n","\n","        token_labels.append(label2id[labels[start_idx]])\n","\n","    length = len(tokenized.input_ids)\n","\n","    return {**tokenized, \"labels\": token_labels, \"length\": length}\n","\n","def compute_metrics(p, all_labels):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    \n","    recall = recall_score(true_labels, true_predictions)\n","    precision = precision_score(true_labels, true_predictions)\n","    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n","    \n","    results = {\n","        'recall': recall,\n","        'precision': precision,\n","        'f1': f1_score\n","    }\n","    return results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:31:49.699691Z","iopub.status.busy":"2024-01-28T09:31:49.699127Z","iopub.status.idle":"2024-01-28T09:31:49.941544Z","shell.execute_reply":"2024-01-28T09:31:49.940633Z","shell.execute_reply.started":"2024-01-28T09:31:49.699653Z"},"trusted":true},"outputs":[],"source":["unique_labels = set(label for item in train_data for label in item[\"labels\"])\n","unique_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:31:49.943849Z","iopub.status.busy":"2024-01-28T09:31:49.943473Z","iopub.status.idle":"2024-01-28T09:55:01.815862Z","shell.execute_reply":"2024-01-28T09:55:01.814829Z","shell.execute_reply.started":"2024-01-28T09:31:49.943816Z"},"trusted":true},"outputs":[],"source":["import json\n","import numpy as np\n","from functools import partial\n","from datasets import load_metric, Dataset\n","from transformers import (\n","    AutoModelForTokenClassification, \n","    TrainingArguments, \n","    Trainer, \n","    DataCollatorForTokenClassification,\n","    AutoTokenizer\n",")\n","from tokenizers import AddedToken\n","import evaluate\n","from itertools import chain\n","\n","    \n","# train_data = train_data[:1000]\n","# Extract and set up label mappings\n","unique_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n","label2id = {label: i for i, label in enumerate(unique_labels)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","\n","# Validate and convert training data into a dataset\n","dataset = Dataset.from_dict({\n","    \"full_text\": [x[\"full_text\"] for x in data],\n","    \"document\": [str(x[\"document\"]) for x in data],\n","    \"tokens\": [x[\"tokens\"] for x in data],\n","    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    \"provided_labels\": [x[\"labels\"] for x in data],\n","})\n","\n","# Training configuration\n","TRAIN_MODEL_PATH = \"microsoft/deberta-v3-base\"\n","MAX_LENGTH = 1024\n","OUTPUT_DIR = \"output\"\n","\n","# Initialize tokenizer and modify for specific use-case\n","tokenizer = AutoTokenizer.from_pretrained(TRAIN_MODEL_PATH)  \n","\n","# Filter dataset and apply tokenization\n","ds = dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id,\"max_length\": MAX_LENGTH}, num_proc=3)\n","\n","\n","# Configure the model\n","model = AutoModelForTokenClassification.from_pretrained(\n","    TRAIN_MODEL_PATH, \n","    num_labels=len(unique_labels), \n","    id2label=id2label, \n","    label2id=label2id,\n","    ignore_mismatched_sizes=True)\n","\n","# Prepare data collator\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n","\n","\n","args = TrainingArguments(\n","    output_dir=OUTPUT_DIR, \n","    fp16=True,\n","    learning_rate=2e-5,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=2,\n","    report_to=\"none\",\n","    evaluation_strategy=\"no\",\n","    do_eval=False,\n","    save_total_limit=1,\n","    logging_steps=20,\n","    lr_scheduler_type='cosine',\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    warmup_ratio=0.1,\n","    weight_decay=0.01\n",")\n","\n","\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=ds,\n","    data_collator=collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=partial(compute_metrics, all_labels=unique_labels),\n",")\n","\n","# Start the training process\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:57:40.621373Z","iopub.status.busy":"2024-01-28T09:57:40.620530Z","iopub.status.idle":"2024-01-28T09:57:42.228717Z","shell.execute_reply":"2024-01-28T09:57:42.227666Z","shell.execute_reply.started":"2024-01-28T09:57:40.621339Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T09:58:06.614232Z","iopub.status.busy":"2024-01-28T09:58:06.613347Z","iopub.status.idle":"2024-01-28T09:58:09.994063Z","shell.execute_reply":"2024-01-28T09:58:09.992999Z","shell.execute_reply.started":"2024-01-28T09:58:06.614198Z"},"trusted":true},"outputs":[],"source":["## inference\n","\n","import json\n","import argparse\n","from itertools import chain\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n","from datasets import Dataset\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","\n","\n","def tokenize(example, tokenizer, max_length):\n","    text, token_map, idx = [], [], 0\n","    for token, has_space in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n","        text.append(token)\n","        token_map.extend([idx] * len(token))\n","        if has_space:\n","            text.append(\" \")\n","            token_map.append(-1)\n","        idx += 1\n","\n","    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n","    return {**tokenized, \"token_map\": token_map}\n","\n","def load_data(file_path):\n","    data = json.load(open(file_path))\n","    return Dataset.from_dict({\n","        \"full_text\": [x[\"full_text\"] for x in data],\n","        \"document\": [x[\"document\"] for x in data],\n","        \"tokens\": [x[\"tokens\"] for x in data],\n","        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n","    })\n","\n","def create_dataframe(preds, ds, id2label):\n","    triplets = []\n","    document, token, label, token_str = [], [], [], []\n","    for p, token_map, offsets, tokens, doc in zip(preds, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n","\n","        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n","            label_pred = id2label[str(token_pred)]\n","\n","            if start_idx + end_idx == 0: continue\n","\n","            if token_map[start_idx] == -1: \n","                start_idx += 1\n","\n","            # ignore \"\\n\\n\"\n","            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n","                start_idx += 1\n","\n","            if start_idx >= len(token_map): break\n","\n","            token_id = token_map[start_idx]\n","\n","            # ignore \"O\" predictions and whitespace preds\n","            if label_pred != \"O\" and token_id != -1:\n","                triplet = (label_pred, token_id, tokens[token_id])\n","\n","                if triplet not in triplets:\n","                    document.append(doc)\n","                    token.append(token_id)\n","                    label.append(label_pred)\n","                    token_str.append(tokens[token_id])\n","                    triplets.append(triplet)\n","\n","    return pd.DataFrame({\n","        \"document\": document,\n","        \"token\": token,\n","        \"label\": label,\n","        \"token_str\": token_str\n","    })\n","\n","# Load and prepare data\n","# model_path = \"/kaggle/input/pii-data-detection-baseline/output/checkpoint-240\"\n","model_path = \"/kaggle/working/output\"\n","max_length = 2048 # Define max_length as needed\n","test_file_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n","ds = load_data(test_file_path)\n","\n","# Tokenization and Model Preparation\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}, num_proc=2)\n","model = AutoModelForTokenClassification.from_pretrained(model_path)\n","collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n","\n","# Training Arguments and Trainer Initialization\n","args = TrainingArguments(\".\", per_device_eval_batch_size=4, report_to=\"none\")\n","trainer = Trainer(model=model, args=args, data_collator=collator, tokenizer=tokenizer)\n","\n","# Prediction and Saving\n","predictions = trainer.predict(ds).predictions\n","np.save(\"preds.npy\", predictions)\n","ds.to_parquet(\"test_ds.pq\")    \n","    \n","# Post-processing Predictions\n","config = json.load(open(Path(model_path) / \"config.json\"))\n","id2label = config[\"id2label\"]\n","preds = np.load(\"preds.npy\").argmax(-1)\n","ds = Dataset.from_parquet(\"test_ds.pq\")\n","\n","\n","# Create results dataframe and dump it in sumbmission.csv file\n","df = create_dataframe(preds, ds, id2label)\n","df[\"row_id\"] = list(range(len(df)))\n","df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n","display(df.head(50))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# import pandas as pd\n","# import plotly.express as px\n","\n","# # Flatten the dataset for analysis\n","# flattened_data = [(label, i) for sample_labels in ds[\"labels\"] for i, label in enumerate(sample_labels) if label != \"O\"]\n","\n","# # Create a DataFrame\n","# df = pd.DataFrame(flattened_data, columns=[\"label\", \"position\"])\n","\n","# # Define group thresholds\n","# bins = [0, 50, 100, 200, 500, 1000, 2000, 10000]\n","# df['range'] = pd.cut(df['position'], bins, right=False)\n","\n","# # Convert Interval objects to strings\n","# df['range'] = df['range'].astype(str)\n","\n","# # Group and count\n","# grouped_df = df.groupby(['label', 'range']).size().reset_index(name='count')\n","\n","# # Plot\n","# px.scatter(grouped_df, x=\"range\", y=\"count\", color=\"label\", log_y=True, height=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"sourceId":160655963,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
