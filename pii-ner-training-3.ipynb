{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":160655963,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q  seqeval evaluate\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:03.948907Z","iopub.execute_input":"2024-01-28T09:31:03.949796Z","iopub.status.idle":"2024-01-28T09:31:21.510103Z","shell.execute_reply.started":"2024-01-28T09:31:03.949763Z","shell.execute_reply":"2024-01-28T09:31:21.508966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q datasets accelerate ","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:21.512487Z","iopub.execute_input":"2024-01-28T09:31:21.512928Z","iopub.status.idle":"2024-01-28T09:31:37.031997Z","shell.execute_reply.started":"2024-01-28T09:31:21.512888Z","shell.execute_reply":"2024-01-28T09:31:37.030993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport json\nimport numpy as np\n\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n\n# downsampling of negative examples\np=[] # positive samples (contain relevant labels)\nn=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\nfor d in data:\n    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n    else: n.append(d)\nprint(\"original datapoints: \", len(data))\n\nexternal = json.load(open(\"/kaggle/input/external-dataset/pii_dataset_fixed.json\"))\nprint(\"external datapoints: \", len(external))\n\nmoredata = json.load(open(\"/kaggle/input/external-dataset/moredata_dataset_fixed.json\"))\nprint(\"moredata datapoints: \", len(moredata))\n\ntrain_data = moredata+external+p+n[:len(n)//3]\nprint(\"combined: \", len(data))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:37.033631Z","iopub.execute_input":"2024-01-28T09:31:37.034496Z","iopub.status.idle":"2024-01-28T09:31:43.108989Z","shell.execute_reply.started":"2024-01-28T09:31:37.034449Z","shell.execute_reply":"2024-01-28T09:31:43.107979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport numpy as np\nimport random\nfrom seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\ndef tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n\n    for start_idx, end_idx in tokenized.offset_mapping:\n        # CLS token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}\n\ndef compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    return results\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:43.111466Z","iopub.execute_input":"2024-01-28T09:31:43.111805Z","iopub.status.idle":"2024-01-28T09:31:49.697756Z","shell.execute_reply.started":"2024-01-28T09:31:43.111778Z","shell.execute_reply":"2024-01-28T09:31:49.696673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = set(label for item in train_data for label in item[\"labels\"])\nunique_labels","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:49.699127Z","iopub.execute_input":"2024-01-28T09:31:49.699691Z","iopub.status.idle":"2024-01-28T09:31:49.941544Z","shell.execute_reply.started":"2024-01-28T09:31:49.699653Z","shell.execute_reply":"2024-01-28T09:31:49.940633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom functools import partial\nfrom datasets import load_metric, Dataset\nfrom transformers import (\n    AutoModelForTokenClassification, \n    TrainingArguments, \n    Trainer, \n    DataCollatorForTokenClassification,\n    AutoTokenizer\n)\nfrom tokenizers import AddedToken\nimport evaluate\nfrom itertools import chain\n\n    \n# train_data = train_data[:1000]\n# Extract and set up label mappings\nunique_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\nlabel2id = {label: i for i, label in enumerate(unique_labels)}\nid2label = {i: label for label, i in label2id.items()}\n\n\n# Validate and convert training data into a dataset\ndataset = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n})\n\n# Training configuration\nTRAIN_MODEL_PATH = \"microsoft/deberta-v3-base\"\nMAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"\n\n# Initialize tokenizer and modify for specific use-case\ntokenizer = AutoTokenizer.from_pretrained(TRAIN_MODEL_PATH)  \n\n# Filter dataset and apply tokenization\nds = dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id,\"max_length\": MAX_LENGTH}, num_proc=3)\n\n\n# Configure the model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    TRAIN_MODEL_PATH, \n    num_labels=len(unique_labels), \n    id2label=id2label, \n    label2id=label2id,\n    ignore_mismatched_sizes=True)\n\n# Prepare data collator\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)\n\n\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds,\n    data_collator=collator,\n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=unique_labels),\n)\n\n# Start the training process\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:31:49.943473Z","iopub.execute_input":"2024-01-28T09:31:49.943849Z","iopub.status.idle":"2024-01-28T09:55:01.815862Z","shell.execute_reply.started":"2024-01-28T09:31:49.943816Z","shell.execute_reply":"2024-01-28T09:55:01.814829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:57:40.620530Z","iopub.execute_input":"2024-01-28T09:57:40.621373Z","iopub.status.idle":"2024-01-28T09:57:42.228717Z","shell.execute_reply.started":"2024-01-28T09:57:40.621339Z","shell.execute_reply":"2024-01-28T09:57:42.227666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## inference\n\nimport json\nimport argparse\nfrom itertools import chain\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n\ndef tokenize(example, tokenizer, max_length):\n    text, token_map, idx = [], [], 0\n    for token, has_space in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        text.append(token)\n        token_map.extend([idx] * len(token))\n        if has_space:\n            text.append(\" \")\n            token_map.append(-1)\n        idx += 1\n\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n    return {**tokenized, \"token_map\": token_map}\n\ndef load_data(file_path):\n    data = json.load(open(file_path))\n    return Dataset.from_dict({\n        \"full_text\": [x[\"full_text\"] for x in data],\n        \"document\": [x[\"document\"] for x in data],\n        \"tokens\": [x[\"tokens\"] for x in data],\n        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    })\n\ndef create_dataframe(preds, ds, id2label):\n    triplets = []\n    document, token, label, token_str = [], [], [], []\n    for p, token_map, offsets, tokens, doc in zip(preds, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[str(token_pred)]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1: \n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            token_id = token_map[start_idx]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.append(triplet)\n\n    return pd.DataFrame({\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n# Load and prepare data\n# model_path = \"/kaggle/input/pii-data-detection-baseline/output/checkpoint-240\"\nmodel_path = \"/kaggle/working/output\"\nmax_length = 2048 # Define max_length as needed\ntest_file_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\nds = load_data(test_file_path)\n\n# Tokenization and Model Preparation\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}, num_proc=2)\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n# Training Arguments and Trainer Initialization\nargs = TrainingArguments(\".\", per_device_eval_batch_size=4, report_to=\"none\")\ntrainer = Trainer(model=model, args=args, data_collator=collator, tokenizer=tokenizer)\n\n# Prediction and Saving\npredictions = trainer.predict(ds).predictions\nnp.save(\"preds.npy\", predictions)\nds.to_parquet(\"test_ds.pq\")    \n    \n# Post-processing Predictions\nconfig = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\npreds = np.load(\"preds.npy\").argmax(-1)\nds = Dataset.from_parquet(\"test_ds.pq\")\n\n\n# Create results dataframe and dump it in sumbmission.csv file\ndf = create_dataframe(preds, ds, id2label)\ndf[\"row_id\"] = list(range(len(df)))\ndf[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\ndisplay(df.head(50))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T09:58:06.613347Z","iopub.execute_input":"2024-01-28T09:58:06.614232Z","iopub.status.idle":"2024-01-28T09:58:09.994063Z","shell.execute_reply.started":"2024-01-28T09:58:06.614198Z","shell.execute_reply":"2024-01-28T09:58:09.992999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import pandas as pd\n# import plotly.express as px\n\n# # Flatten the dataset for analysis\n# flattened_data = [(label, i) for sample_labels in ds[\"labels\"] for i, label in enumerate(sample_labels) if label != \"O\"]\n\n# # Create a DataFrame\n# df = pd.DataFrame(flattened_data, columns=[\"label\", \"position\"])\n\n# # Define group thresholds\n# bins = [0, 50, 100, 200, 500, 1000, 2000, 10000]\n# df['range'] = pd.cut(df['position'], bins, right=False)\n\n# # Convert Interval objects to strings\n# df['range'] = df['range'].astype(str)\n\n# # Group and count\n# grouped_df = df.groupby(['label', 'range']).size().reset_index(name='count')\n\n# # Plot\n# px.scatter(grouped_df, x=\"range\", y=\"count\", color=\"label\", log_y=True, height=1000)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}